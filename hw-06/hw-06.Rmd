---
title: "36-402 Homework 6"
author:
- Eu Jing Chua
- eujingc
date: "February 25, 2019"
output:
  pdf_document: default
header-includes:
    - \usepackage{enumerate}
    - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\newcommand{\est}[1]{\hat{#1}}
\newcommand{\betah}[1]{\est{\beta_{#1}}}
\newcommand{\avg}[1]{\overline{#1}}
\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var} \left[ #1 \right]}
\newcommand{\Cov}[1]{\text{Cov} \left[ #1 \right]}
\newcommand{\X}{\mathbb{X}}
\newcommand{\sumTo}[1]{\sum^{#1}_{i=1}}
\newcommand{\sumjTo}[1]{\sum^{#1}_{j=1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\argmin}[2]{\mathop{\mathrm{argmin}}_{#1} \left[ #2 \right]}

```{r}
library(knitr)
library(np)
options(np.messages = FALSE)
library(mgcv)
```

```{r}
gmp <- read.csv("http://www.stat.cmu.edu/~cshalizi/uADA/17/hw/05/gmp-2006.csv")
```

# Question 1

**Q1 a)**

```{r cache=TRUE}
lm.fit <- lm(log(pcgmp) ~ log(pop), data = gmp)
npreg.fit <- npreg(log(pcgmp) ~ log(pop), data = gmp)
```

```{r}
insamp.mse <- function (model) {
    return(mean(resid(model) ^ 2))
}
```

```{r}
results <- c(insamp.mse(lm.fit), insamp.mse(npreg.fit))
names(results) <- c("Power-law Model", "NP Regression")
kable(results, digits = 5, caption = "In-sample MSE")
```

**Q1 b)**

```{r}
# From page 144 of textbook
resample <- function(x) {
    sample(x, size = length(x), replace = TRUE)
}
```

```{r cache=TRUE}
powerlaw.sim <- function(pop) {
    new.df <- data.frame(pop = pop)
    new.log.pcgmp <- predict(lm.fit, newdata = new.df) + resample(resid(lm.fit))
    new.df$pcgmp <- exp(new.log.pcgmp)
    return(new.df)
}
mse.difference <- function(new.gmp) {
    lm.fit <- lm(log(pcgmp) ~ log(pop), data = new.gmp)
    npreg.fit <- npreg(log(pcgmp) ~ log(pop), data = new.gmp)
    return(insamp.mse(lm.fit) - insamp.mse(npreg.fit))
}
null.samples.D <- replicate(200, mse.difference(powerlaw.sim(gmp$pop)))
D.hat <- insamp.mse(lm.fit) - insamp.mse(npreg.fit)
```

```{r}
kable(mean(null.samples.D >= D.hat),
      caption = "Probability of observed MSE difference under Power-law Model")
```

**Q1 c)**

Assuming we are conducting this hypothesis test at 5% significance, then we conclude that we do not reject that the true model follows a power law, as we have insufficient evidence to reject that.

# Question 2

**Q2 a)**

```{r fig.height = 6}
am.fit <- gam(log(pcgmp) ~ s(finance) + s(prof.tech) + s(ict) + s(management), data = gmp)
par(mfrow = c(2, 2))
plot(am.fit)
```

The partial responses of finance, prof.tech, and management all look quite linear with positive slopes. However, the partial response of ict does not look linear but is concave up and increasing.

**Q2 b)**
```{r}
am.fit.2 <- gam(log(pcgmp) ~ s(finance) + s(prof.tech) + s(ict) + s(management) + log(pop), data = gmp)
kable(coef(am.fit.2)[2], digits = 5, caption = "Coefficient")
```

```{r fig.height = 6}
par(mfrow = c(2, 2))
plot(am.fit.2)
```

It seems that the partial response functions did not change much with the additional term added.


**Q2 c)**

```{r}
# Taken from page 136 of textbook
rboot <- function(statistic, simulator, B) {
    tboots <- replicate(B, statistic(simulator()))
    if (is.null(dim(tboots))) {
        tboots <- array(tboots, dim = c(1, B))
    }
    return(tboots)
}

bootstrap <- function(tboots, summarizer, ...) {
    summaries <- apply(tboots, 1, summarizer, ...)
    return(t(summaries))
}

# From page 138 of textbook
equitails <- function(x, alpha) {
    lower <- quantile(x, alpha/2)
    upper <- quantile(x, 1 - alpha/2)
    return(c(lower, upper))
}
bootstrap.ci <- function(statistic = NULL, simulator = NULL, tboots = NULL,
    B = if (!is.null(tboots)) {
        ncol(tboots)
    }, t.hat, level) {

    if (is.null(tboots)) {
        stopifnot(!is.null(statistic))
        stopifnot(!is.null(simulator))
        stopifnot(!is.null(B))
        tboots <- rboot(statistic, simulator, B)
    }

    alpha <- 1 - level
    intervals <- bootstrap(tboots, summarizer = equitails, alpha = alpha)
    upper <- t.hat + (t.hat - intervals[, 1])
    lower <- t.hat + (t.hat - intervals[, 2])
    CIs <- cbind(lower = lower, upper = upper)
    return(CIs)
}
resample.data.frame <- function(data) {
    sample.rows <- resample(1:nrow(data))
    return(data[sample.rows, ])
}
```

```{r cache = TRUE}
coef.estimator <- function(data) {
    new.am.fit <- gam(log(pcgmp) ~ s(finance) + s(prof.tech) + s(ict) + s(management) + log(pop), data = data)
    return(coef(new.am.fit)[2])
}
resample.cases <- function() {
    resample.data.frame(gmp)
}
am.coef.cases.cis <- bootstrap.ci(statistic = coef.estimator,
                                  simulator = resample.cases,
                                  B = 1000,
                                  t.hat = coef(am.fit.2)[2],
                                  level = 0.95)
```

For the bootstrap, we use case-resampling of the original data to generate new samples to produce an estimate of the following:

```{r}
kable(am.coef.cases.cis, digits = 5,
      caption = "95% C.I. for Coefficient")
```

**Q2 d)**

The CI from before does include 0. We can conclude that power-law scaling with population size does not seem to be significant in the presence of the other smoothed predictors. Also, this also shows that the power-law model may not be strictly supra-linear scaling, as the confidence interval does not only have positive values in it.

**Q2 e)**

```{r}
am.fit.3 <- gam(log(pcgmp) ~ s(finance) + s(prof.tech) + s(ict) + s(management) + s(log(pop)), data = gmp)
```

```{r fig.height = 7}
par(mfrow = c(3, 2))
plot(am.fit.3)
```

The partial response functions of finance, prof.tech and ict does not seem to have changed much. However, the function for management is now more non-linear, generally increasing while being concave down then concave up.

The partial response function of log(pop) is non-linear, being roughly concave up where it decreases up till around 13 log population size, before increasing again.

**Q2 f)**

It seems that population size is a weak but real determinant of a city's per-capita output. In the model of 2 d), we see that in the presence of the other smoothed predictors, the coefficient for the linear term of log(pop) is non-significant. However, when we model additive model with a smoothed log(pop) term as in 2 e), we see that it has a partial response function that seems significantly non-zero for certain portions such as lower values of log population from 11 to 11.5, as seen from the 2 standard error band around it.

The idea of strictly supra-linear scaling is also not supported, as the coefficient of log(pop) is not strictly positive. Hence, it is weak but still acts as a determinant of a city's per-capita output.

# Question 3

```{r cache=TRUE}
powerlaw.lm.sim <- function(null.lm.fit, data) {
    new.df <- na.omit(data)
    new.log.pcgmp <- predict(null.lm.fit, newdata = new.df) + resample(resid(null.lm.fit))
    new.df$pcgmp <- exp(new.log.pcgmp)
    return(new.df)
}
mse.difference <- function(new.gmp) {
    lm.fit <- lm(log(pcgmp) ~ finance + prof.tech + ict + management + log(pop), data = new.gmp)
    am.fit <- gam(log(pcgmp) ~ s(finance) + s(prof.tech) + s(ict) + s(management) + s(log(pop)), data = new.gmp)
    return(insamp.mse(lm.fit) - insamp.mse(am.fit))
}
lm.fit.2 <- lm(log(pcgmp) ~ finance + prof.tech + ict + management + log(pop), data = gmp)
null.samples.D.2 <- replicate(200, mse.difference(powerlaw.lm.sim(lm.fit.2, gmp)))
D.hat.2 <- insamp.mse(lm.fit.2) - insamp.mse(am.fit.3)
```

```{r}
kable(mean(null.samples.D.2 >= D.hat.2),
      caption = "Probability of observed MSE difference under linear model")
```

# Question 4

**Q4 a)**
\begin{align}
\mu_j(x_j) &= \E{Y \mid X_j = x_j} \\
    &= \alpha + \sum_{k = 1}^p \E{f_k(X_k) \mid X_j = x_j} + 0 \\
    &= \alpha + f_j(x_j) + \sum_{k \ne j}^p \E{f_k(X_k) \mid X_j = x_j}
\end{align}

**Q4 b)**
Since $X_k$ is independent of $X_j$ for $k \ne j$, we know that $\E{f_k(X_k) \mid X_j = x_j} = \E{f_k(X_k)} = 0$.
\begin{align}
\mu_j(x_j) &= \alpha + f_j(x_j) + \sum_{k \ne j}^p \E{f_k(X_k) \mid X_j = x_j} \\
    &= \alpha + f_j(x_j) \\
\mu_j(x_j) - \alpha &= f_j(x_j)
\end{align}

**Q4 c)**
Consider a model where $Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \epsilon$, where $X_1 = X_2$.  
If 4b) applied in this case, we would find that $\mu_1(x_1) = \alpha + \beta_1 x_1$.  
However, since $X_1 = X_2$, $\E{\beta_2 X_2 \mid X_1 = x_1} = \beta_2 x_1$, so it should be that $\mu_1(x_1) = \alpha + \beta_1 x_1 + \beta_2 x_1$.

Hence, 4b) does not hold if some $X_k$ is statistically dependent of $X_j$.

# Question 5

**Q5 a)**

\begin{align}
    \est{\beta}_{RR} &= \argmin{\beta}{ \left[ \frac{1}{n} \sumTo{n} (y_i - x_i \cdot \beta)^2 \right] + \lambda \sumjTo{p} \beta_j^2} \\
        &= \argmin{\beta}{n^{-1} (\matr{y} - \matr{x} \beta)^T(\matr{y} - \matr{x} \beta) + \lambda \beta^T \beta}
\end{align}

**Q5 b)**

\begin{align}
    \est{\beta}_{RR}
        &= \argmin{\beta}{n^{-1} (\matr{y} - \matr{x} \beta)^T(\matr{y} - \matr{x} \beta) + \lambda \beta^T \beta} \\
    0 &= \frac{\partial}{\partial \beta} n^{-1} (\matr{y} - \matr{x} \beta)^T(\matr{y} - \matr{x} \beta) + \lambda \beta^T \beta \\
        &= - n^{-1} 2 \matr{x}^T(\matr{y} - \matr{x} \beta) + 2 \lambda \beta \\
    n^{-1} \matr{x}^T \matr{y} &= n^{-1} \matr{x}^T \matr{x} \beta + \lambda \beta \\
    \matr{x}^T \matr{y} &= \matr{x}^T \matr{x} \beta + n \lambda \matr{I} \beta \\
    \matr{x}^T \matr{y} &= (\matr{x}^T \matr{x} + n \lambda \matr{I}) \beta \\
    \implies \est{\beta}_{RR} &= (\matr{x}^T \matr{x} + n \lambda \matr{I})^{-1} \matr{x}^T \matr{y}
\end{align}

**Q5 c)**

As $\lambda \rightarrow 0$, $\est{\beta}_{RR} \rightarrow (\matr{x}^T \matr{x})^{-1}\matr{x}^T \matr{y} = \est{\beta}_{OLS}$, we reduce to linear regression by OLS.

We know that $\matr{x}^T\matr{x}$ is symmetric and positive semi-definite, hence $\matr{x}^T\matr{x} + n\lambda \matr{I}$ is positive definite for $\lambda > 0$.
Given that $\matr{x}^T \matr{x}$ has eigenvalues $\lambda_i \ge 0$, $i = 1 \hdots p$, the positive definite matrix $\matr{x}^T \matr{x} + n\lambda \matr{I}$ has eigenvalues $\lambda_i + \lambda > 0$.

As $\lambda \rightarrow \infty$, the eigenvalues $\lambda_i + \lambda \rightarrow \infty$. Then the inverse of the positive definite matrix has eigenvalues $\frac{1}{\lambda_i + \lambda} \rightarrow 0$, so $(\matr{x}^T \matr{x} + n\lambda \matr{I})^{-1} \rightarrow \matr{0}$. Thus, this causes $\est{\beta}_{RR} \rightarrow \matr{0}$.

**Q5 d)**

```{r fig.height = 6}
library(glmnet)
library(latex2exp)
Z <- runif(2000, min = -1, max = 1)
epsilon <- rnorm(2000, sd = sqrt(0.05))
Y <- Z + epsilon
X <- replicate(50, 0.9 * Z + rnorm(2000, sd = sqrt(0.05)))
trainX <- X[1:1000, ]
trainY <- Y[1:1000]
testX <- X[1001:2000, ]
testY <- Y[1001:2000]

rr.fit <- glmnet(trainX, trainY, alpha = 0)
matplot(rr.fit$lambda, t(coef(rr.fit)[2:51, ]), type = "l", lty = 1,
        xlab = TeX("$\\lambda$"), ylab = "Coefficient",
        main = TeX("Plot of 50 coefficients of $X_i$ against $\\lambda$"))
abline(h = 0, col = "grey")
```

Ridge regression is a shrinkage estimator as increasing the penalty $\lambda$ causes the estimated coefficients of the model to "shrink" towards 0.

**Q5 e)**

```{r}
cv.rr.fit <- cv.glmnet(trainX, trainY, alpha = 0)
lm.fit <- lm(trainY ~ trainX)

outsamp.mse.rr <- mean((predict(cv.rr.fit, s = cv.rr.fit$lambda.min, newx = testX) - testY)^2)
outsamp.mse.lm <- mean((predict(lm.fit, newx = testX) - testY)^2)

outsamp.mse <- c(outsamp.mse.rr, outsamp.mse.lm)
names(outsamp.mse) <- c("Ridge Regression", "OLS")
kable(outsamp.mse, digits = 5,
      caption = "Out-sample MSE")
```

As seen from the MSEs above, the out-of-sample performance of the Ridge Regression is better than the OLS Regression, having a much lower MSE.
