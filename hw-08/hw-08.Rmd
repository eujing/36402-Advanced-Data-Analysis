---
title: "36-402 Homework 8"
author:
- Eu Jing Chua
- eujingc
date: "April 02, 2019"
output:
  pdf_document: default
header-includes:
    - \usepackage{enumerate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\newcommand{\est}[1]{\hat{#1}}
\newcommand{\betah}[1]{\est{\beta_{#1}}}
\newcommand{\avg}[1]{\overline{#1}}
\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var} \left[ #1 \right]}
\newcommand{\Cov}[1]{\text{Cov} \left[ #1 \right]}
\newcommand{\X}{\mathbb{X}}
\newcommand{\sumTo}[1]{\sum^{#1}_{i=1}}
\newcommand{\sumjTo}[1]{\sum^{#1}_{j=1}}
\newcommand{\matr}[1]{\mathbf{#1}}

```{r}
library(knitr)
library(np)
options(np.messages = FALSE)

load("stockData.RData")
```

# Question 1

**a)**

```{r}
dates <- as.Date(rownames(close_price))
matplot(dates, close_price, type = "l", lty = 1,
        xaxt = "n", xlab = "Dates in 2015", ylab = "Closing Prices",
        main = "Plot of closing prices against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

**b)**

```{r echo=TRUE}
n <- nrow(close_price)
l.returns <- log(close_price[2:n, ] / close_price[1:(n-1), ])
```

**c)**

```{r}
matplot(dates[2:n], l.returns, type = "l", lty = 1,
        col = adjustcolor(1:6, alpha.f = 0.4),
        xaxt = "n", xlab = "Dates in 2015", ylab = "Log Returns",
        main = "Plot of log returns against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

The log returns look more comparable as they all have the same scale now and the dependence over time is more visible.

\newpage

# Question 2

**a)**

```{r fig.height = 3.5}
hist(l.returns[, "GE"], breaks = 30,
     xlab = "Log Returns",
     main = "Histogram of log returns for GE")
```

**b)**

```{r fig.height = 3.5}
hist(l.returns[, "GE"], breaks = 30, probability = TRUE,
     xlab = "Log Returns",
     main = "Histogram of log returns for GE")
curve(dnorm(x,
            mean = mean(l.returns[, "GE"]),
            sd = sd(l.returns[, "GE"])), add = TRUE)
```

The normal distribution appears to fit the distribution of the log returns well, with the exception of possible outliers with exceptionally high log returns.
**c)**

```{r}
np.lr.ge <- npudens(l.returns$GE)
plot(np.lr.ge, col = "red",
     xlab = "Log Returns", main = "Plot of Density of Log Returns of GE")
curve(dnorm(x,
            mean = mean(l.returns[, "GE"]),
            sd = sd(l.returns[, "GE"])),
      add = TRUE, col = "blue")
legend("topright",
       legend = c(paste("KDE (bw = ", signif(np.lr.ge$bw, 3), ")"), "Gaussian"),
       lty = 1, col = c("red", "blue"))
```

The kernel density estimate (KDE) has slightly heavier tails than the best-fitting gaussian and is also slightly skewed to the left. The KDE also has relatively lower density around the mean. However, it still appears approximately gaussian on the whole.

**d)**

```{r cache = TRUE}
cols <- 1:6
sorted.idx <- order(l.returns[, 1])
plot(l.returns[sorted.idx, 1], fitted(npudens(l.returns[, 1]))[sorted.idx],
     xlim = c(min(l.returns), max(l.returns)),
     ylim = c(0, 50),
     type = "l", col = cols[1],
     xlab = "Log Returns", ylab = "Density",
     main = "Plot of Density of Log Returns")
for (i in 2:28) {
    sorted.idx <- order(l.returns[, i])
    lines(l.returns[sorted.idx, i], fitted(npudens(l.returns[, i]))[sorted.idx],
         col = cols[((i - 1) %% 6) + 1])
}
```

The curves seem similar to that of the GE curve, and supports the previous statements as the curve all look approximately gaussian, with mean near 0.

# Question 3

**a)**

```{r}
one.factor <- factanal(l.returns, factors = 1, scores = "regression")
kable(signif(one.factor$loadings[, 1], 3),
      col.names = "w",
      caption = "Table of loadings for one-factor model")
```

It makes sense that there are 28 entries as we only have 1 row of loadings by setting the factors to 1, where there are 28 columns for the matrix of loadings.

**b)**

```{r}
barplot(sort(one.factor$loadings[, 1]), las = 1,
        xlab = "Company", ylab = "Loading",
        main = "Plot of loading by company")
```

The minimum loading is around 0.50, while the maximum loading is around 0.85. Most of the loadings are also around a narrow range from 0.6 to 0.7.

```{r}
plot(dates[2:n], one.factor$scores, type = "l",
     xlab = "Dates in 2015", ylab = "Factor score", xaxt = "n",
     main = "Plot of factor score against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

Most of the factor scores seem to be randomly distributed over time within -2 and 2. However, we can observe a large dip and spike in late August, going from roughly -4 to 4.

**d)**

The time-series plot of factor scores summarizes the time-seies plot of log-returns for each company well, smoothing out the variations between each company. Both plots show similar trends, with scores and log returns looking roughly stationary around 0 across time. The general large dip and spike across all companies' log returns around late August is also clearly reflected in the factor score plot.

# Question 4

```{r}
# From page 144 of textbook
resample <- function(x) {
    sample(x, size = length(x), replace = TRUE)
}
# Taken from page 136 of textbook
rboot <- function(statistic, simulator, B) {
    tboots <- replicate(B, statistic(simulator()))
    if (is.null(dim(tboots))) {
        tboots <- array(tboots, dim = c(1, B))
    }
    return(tboots)
}

bootstrap <- function(tboots, summarizer, ...) {
    summaries <- apply(tboots, 1, summarizer, ...)
    return(t(summaries))
}

# From page 138 of textbook
equitails <- function(x, alpha) {
    lower <- quantile(x, alpha/2)
    upper <- quantile(x, 1 - alpha/2)
    return(c(lower, upper))
}
bootstrap.ci <- function(statistic = NULL, simulator = NULL, tboots = NULL,
    B = if (!is.null(tboots)) {
        ncol(tboots)
    }, t.hat, level) {

    if (is.null(tboots)) {
        stopifnot(!is.null(statistic))
        stopifnot(!is.null(simulator))
        stopifnot(!is.null(B))
        tboots <- rboot(statistic, simulator, B)
    }

    alpha <- 1 - level
    intervals <- bootstrap(tboots, summarizer = equitails, alpha = alpha)
    upper <- t.hat + (t.hat - intervals[, 1])
    lower <- t.hat + (t.hat - intervals[, 2])
    CIs <- cbind(lower = lower, upper = upper)
    return(CIs)
}
resample.data.frame <- function(data) {
    sample.rows <- resample(1:nrow(data))
    return(data[sample.rows, ])
}
```

```{r cache = TRUE}
loadings.simulator <- function(new.data) {
    new.one.factor <- factanal(new.data, factors = 1, scores = "regression")
    return(new.one.factor$loadings[, 1])
}

resample.cases <- function() {
    resample.data.frame(l.returns)
}

one.factor.loadings.cis <- bootstrap.ci(statistic = loadings.simulator,
                                        simulator = resample.cases,
                                        B = 300, level = 0.90,
                                        t.hat = one.factor$loadings[, 1])
```

```{r}
sorted.idx <- order(one.factor$loadings[, 1])
x <- 1:28
plot(x, one.factor$loadings[sorted.idx, 1], type = "p", pch = 16,
     ylim = c(min(one.factor.loadings.cis[, 1]),
              max(one.factor.loadings.cis[, 2])),
     xlab = "Company", ylab = "Loadings", xaxt = "n",
     main = "Plot of loadings (with error bars) against company")
lab.idx <- seq(from = 1, to = 28, by = 3)
axis(1, lab.idx, names(one.factor$loadings[sorted.idx, 1])[lab.idx])
arrows(x,
       one.factor.loadings.cis[sorted.idx, 1],
       x,
       one.factor.loadings.cis[sorted.idx, 2],
       length = 0.05, angle = 90, code = 3)
```

\newpage

# Question 5

**a)**

```{r}
cov.GE.CVX <- cov(l.returns[, "GE"], l.returns[, "CVX"])
cov.GE.BA <- cov(l.returns[, "GE"], l.returns[, "BA"])
cov.GS.CVX <- cov(l.returns[, "GS"], l.returns[, "CVX"])
cov.GS.BA <- cov(l.returns[, "GS"], l.returns[, "BA"])
results <- matrix(c(cov.GE.CVX, cov.GE.BA, cov.GS.CVX, cov.GE.BA),
                  nrow = 2, ncol = 2, byrow = TRUE)
rownames(results) <- c("GE", "GS")
colnames(results) <- c("CVX", "BA")
kable(signif(results, 3), caption = "Covaiances in log returns")
```

**b)**

In the one-factor model, we know that $\Cov{X_i, X_j} = w_i w_j$, where $w_i, w_j$ are the respective loadings for each variable. Hence, we have that
\begin{align*}
\frac{\Cov{GE, CVX} / \Cov{GE, BA}}{\Cov{GS, CVX} / \Cov{GS, BA}} &=
    \frac{w_{GE} w_{CVX}}{w_{GE} w_{BA}} \frac{w_{GS} w_{BA}}{w_{GS} w_{CVX}} \\
    &= \frac{w_{CVX}}{w_{BA}} \frac{w_{BA}}{w_{CVX}} \\
    &= 1
\end{align*}

**c)**

```{r}
obs.cov.ratio <- (cov.GE.CVX / cov.GE.BA) / (cov.GS.CVX / cov.GS.BA)

cov.ratio.simulator <- function(new.data) {
    ratio <- (cov(new.data[, "GE"], new.data[, "CVX"]) / cov(new.data[, "GE"], new.data[, "BA"])) /
            (cov(new.data[, "GS"], new.data[, "CVX"]) / cov(new.data[, "GS"], new.data[, "BA"]))
    return(ratio)
}

cov.ratio.cis <- bootstrap.ci(statistic = cov.ratio.simulator,
                              simulator = resample.cases,
                              B = 300, level = 0.90,
                              t.hat = obs.cov.ratio)
rownames(cov.ratio.cis) <- NULL
```
```{r}
kable(signif(cov.ratio.cis, 3), caption = "90% C.I for covariance ratio")
```

Since the 90% C.I for the ratio contains 1, we can conclude with 90% confidence that there is insufficient evidence from the data that the one-factor model is wrong.

\newpage

# Question 6

```{r fig.height = 6}
two.factor <- factanal(l.returns, factors = 2, scores = "regression")
company.order <- order(one.factor$loadings[, 1])
barplot(two.factor$loadings[company.order, 1], las = 1,
        ylim = c(0.0, max(two.factor$loadings)),
        col = rgb(0, 0, 1, 0.5),
        xlab = "Company", ylab = "Loading",
        main = "Plot of loading of 1st factor by company")
barplot(two.factor$loadings[company.order, 2], las = 1,
        col = rgb(1, 0, 0, 0.5), add = TRUE)
legend("topleft", legend = c("1st Factor", "2nd Factor"),
       col = c("blue", "red"), pch = 15)
```

The bar plot features the companies in the same order as in Q3b)

The loading of the 1st factor for most companies is around the narrow range of 0.5 to 0.7, with 4 companies having lower loadings between 0.2 to 0.4. It should also be noted that the loading of this 1st factor has changed from the loadings in the one-factor model, being lower in general. This decrement in 1st factor affects each company differently, some more than others.

The loading of the 2nd factor for most companies is aound the narrow range of 0.2 to 0.5, with 3 companies having higher loadings above 0.5. In general, we can see that the second set of loadings are smaller than the first set. However, an interesting pattern is present where companies with lower-than-usual 1st factor loadings have higher-than-usual 2nd factor loadings.

\newpage

# Question 7

```{r}
n <- nrow(tricky_prices)
l.returns.2 <- log(tricky_prices[2:n, ] / tricky_prices[1:(n-1), ])
matplot(dates[2:n], l.returns.2, type = "l", lty = 1,
        col = adjustcolor(1:6, alpha.f = 0.4),
        xaxt = "n", xlab = "Dates in 2015", ylab = "Log Returns",
        main = "Plot of log returns against dates in 2015")
axis(1, dates[2:n], format(dates[2:n], "%b %d"))
one.factor.2 <- factanal(l.returns.2, factors = 1, scores = "regression")
new.company.order <- c(company.order, 29, 30)
barplot(one.factor$loadings[company.order, 1], las = 1,
        col = rgb(0, 0, 1, 0.5),
        xlab = "Companies", ylab = "Loading",
        main = "Plot of loadings by company")
barplot(one.factor.2$loadings[new.company.order, 1], las = 1,
        col = rgb(1, 0, 0, 0.5), add = TRUE)

plot(dates[2:n], one.factor.2$scores, type = "l", col = "red",
     xlab = "Dates in 2015", ylab = "Factor score", xaxt = "n",
     main = "Plot of factor score against dates in 2015")
lines(dates[2:n], one.factor$scores)
legend("topleft", legend = c("Original", "With 2 new companies"),
       lty = 1, col = c("red", "black"))
axis(1, dates[2:n], format(dates[2:n], "%b %d"))
```

From timeseries plot of log returns, we can see that each of the two companies added have a large anomaly, represented by the large negative spike in log returns around March and December.

However, once we fit a new one-factor model to his dataset with the two companies added, we find that the loadings of all the previous companies are exactly the same, as shown by the barplot. The two new companies, have relatively lower loadings compared to the previous et of companies. It seems as though the addition of these 2 companies did not affect the previous loadings much.

This is confirmed by the timeseries plot of factor score, which did not change much from the original plot. Since the two new companies have log returns over time that have large anomalies from the general pattern of the other companies, we find that they have low loadings in the one-factor model as they do notreflect a similar measurement of the pattern the other companies exhibit. Since the loadings of the other companies did not change and the two new companies have low loadings, we find that the factor score over time is not affected much by these two new companies, mainly due to their low loading, mainly due to their low loadings.
