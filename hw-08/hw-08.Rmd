---
title: "36-402 Homework 8"
author:
- Eu Jing Chua
- eujingc
date: "April 02, 2019"
output:
  pdf_document: default
header-includes:
    - \usepackage{enumerate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\newcommand{\est}[1]{\hat{#1}}
\newcommand{\betah}[1]{\est{\beta_{#1}}}
\newcommand{\avg}[1]{\overline{#1}}
\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var} \left[ #1 \right]}
\newcommand{\Cov}[1]{\text{Cov} \left[ #1 \right]}
\newcommand{\X}{\mathbb{X}}
\newcommand{\sumTo}[1]{\sum^{#1}_{i=1}}
\newcommand{\sumjTo}[1]{\sum^{#1}_{j=1}}
\newcommand{\matr}[1]{\mathbf{#1}}

```{r}
library(knitr)
library(np)
options(np.messages = FALSE)

load("stockData.RData")
```

# Question 1

**a)**

```{r}
dates <- as.Date(rownames(close_price))
matplot(dates, close_price, type = "l", lty = 1,
        xaxt = "n", xlab = "Dates in 2015", ylab = "Closing Prices",
        main = "Plot of closing prices against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

**b)**

```{r echo=TRUE}
n <- nrow(close_price)
l.returns <- log(close_price[2:n, ] / close_price[1:(n-1), ])
```

**c)**

```{r}
matplot(dates[2:n], l.returns, type = "l", lty = 1,
        col = adjustcolor(1:6, alpha.f = 0.4),
        xaxt = "n", xlab = "Dates in 2015", ylab = "Log Returns",
        main = "Plot of log returns against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

The log returns look more comparable as they all have the same scale now and the dependence over time is more visible.

\newpage

# Question 2

**a)**

```{r fig.height = 3.5}
hist(l.returns[, "GE"], breaks = 30,
     xlab = "Log Returns",
     main = "Histogram of log returns for GE")
```

**b)**

```{r fig.height = 3.5}
hist(l.returns[, "GE"], breaks = 30, probability = TRUE,
     xlab = "Log Returns",
     main = "Histogram of log returns for GE")
curve(dnorm(x,
            mean = mean(l.returns[, "GE"]),
            sd = sd(l.returns[, "GE"])), add = TRUE)
```

The normal distribution appears to fit the distribution of the log returns well, with the exception of possible outliers with exceptionally high log returns.
**c)**

```{r}
np.lr.ge <- npudens(l.returns$GE)
plot(np.lr.ge, col = "red",
     xlab = "Log Returns", main = "Plot of Density of Log Returns of GE")
curve(dnorm(x,
            mean = mean(l.returns[, "GE"]),
            sd = sd(l.returns[, "GE"])),
      add = TRUE, col = "blue")
legend("topright",
       legend = c(paste("KDE (bw = ", signif(np.lr.ge$bw, 3), ")"), "Gaussian"),
       lty = 1, col = c("red", "blue"))
```

The kernel density estimate (KDE) has slightly heavier tails than the best-fitting gaussian and is also slightly skewed to the left. The KDE also has relatively lower density around the mean. However, it still appears approximately gaussian on the whole.

**d)**

```{r cache = TRUE}
cols <- 1:6
sorted.idx <- order(l.returns[, 1])
plot(l.returns[sorted.idx, 1], fitted(npudens(l.returns[, 1]))[sorted.idx],
     xlim = c(min(l.returns), max(l.returns)),
     ylim = c(0, 50),
     type = "l", col = cols[1],
     xlab = "Log Returns", ylab = "Density",
     main = "Plot of Density of Log Returns")
for (i in 2:28) {
    sorted.idx <- order(l.returns[, i])
    lines(l.returns[sorted.idx, i], fitted(npudens(l.returns[, i]))[sorted.idx],
         col = cols[((i - 1) %% 6) + 1])
}
```

The curves seem similar to that of the GE curve, and supports the previous statements as the curve all look approximately gaussian, with mean near 0.

# Question 3

**a)**

```{r}
one.factor <- factanal(l.returns, factors = 1, scores = "regression")
kable(signif(one.factor$loadings[, 1], 3),
      col.names = "w",
      caption = "Table of loadings for one-factor model")
```

It makes sense that there are 28 entries as we only have 1 row of loadings by setting the factors to 1, where there are 28 columns for the matrix of loadings.

**b)**

```{r}
barplot(sort(one.factor$loadings[, 1]), las = 1,
        xlab = "Company", ylab = "Loading",
        main = "Plot of loading against company")
```

The minimum loading is around 0.50, while the maximum loading is around 0.85. Most of the loadings are also around a small range from 0.6 to 0.7.

```{r}
plot(dates[2:n], one.factor$scores, type = "l",
     xlab = "Dates in 2015", ylab = "Factor score", xaxt = "n",
     main = "Plot of factor score against dates in 2015")
axis(1, dates, format(dates, "%b %d"))
```

Most of the factor scores seem to be randomly distributed over time within -2 and 2. However, we can observe a large dip and spike in late August, going from roughly -4 to 4.

**d)**

The time-series plot of factor scores summarizes the time-seies plot of log-returns for each company well, smoothing out the variations between each company. Both plots show similar trends, with scores and log returns looking roughly stationary around 0 across time. The general large dip and spike across all companies' log returns around late August is also clearly reflected in the factor score plot.

# Question 4

```{r}
# From page 144 of textbook
resample <- function(x) {
    sample(x, size = length(x), replace = TRUE)
}
# Taken from page 136 of textbook
rboot <- function(statistic, simulator, B) {
    tboots <- replicate(B, statistic(simulator()))
    if (is.null(dim(tboots))) {
        tboots <- array(tboots, dim = c(1, B))
    }
    return(tboots)
}

bootstrap <- function(tboots, summarizer, ...) {
    summaries <- apply(tboots, 1, summarizer, ...)
    return(t(summaries))
}

# From page 138 of textbook
equitails <- function(x, alpha) {
    lower <- quantile(x, alpha/2)
    upper <- quantile(x, 1 - alpha/2)
    return(c(lower, upper))
}
bootstrap.ci <- function(statistic = NULL, simulator = NULL, tboots = NULL,
    B = if (!is.null(tboots)) {
        ncol(tboots)
    }, t.hat, level) {

    if (is.null(tboots)) {
        stopifnot(!is.null(statistic))
        stopifnot(!is.null(simulator))
        stopifnot(!is.null(B))
        tboots <- rboot(statistic, simulator, B)
    }

    alpha <- 1 - level
    intervals <- bootstrap(tboots, summarizer = equitails, alpha = alpha)
    upper <- t.hat + (t.hat - intervals[, 1])
    lower <- t.hat + (t.hat - intervals[, 2])
    CIs <- cbind(lower = lower, upper = upper)
    return(CIs)
}
resample.data.frame <- function(data) {
    sample.rows <- resample(1:nrow(data))
    return(data[sample.rows, ])
}
```

```{r cache = TRUE}
loadings.simulator <- function(new.data) {
    new.one.factor <- factanal(new.data, factors = 1, scores = "regression")
    return(new.one.factor$loadings[, 1])
}

resample.cases <- function() {
    resample.data.frame(l.returns)
}

one.factor.loadings.cis <- bootstrap.ci(statistic = loadings.simulator,
                                        simulator = resample.cases,
                                        B = 300, level = 0.90,
                                        t.hat = one.factor$loadings[, 1])
```

```{r}
sorted.idx <- order(one.factor$loadings[, 1])
x <- 1:28
plot(x, one.factor$loadings[sorted.idx, 1], type = "p", pch = 16,
     ylim = c(min(one.factor.loadings.cis[, 1]),
              max(one.factor.loadings.cis[, 2])),
     xlab = "Company", ylab = "Loadings", xaxt = "n",
     main = "Plot of loadings (with error bars) against company")
lab.idx <- seq(from = 1, to = 28, by = 3)
axis(1, lab.idx, names(one.factor$loadings[sorted.idx, 1])[lab.idx])
arrows(x,
       one.factor.loadings.cis[sorted.idx, 1],
       x,
       one.factor.loadings.cis[sorted.idx, 2],
       length = 0.05, angle = 90, code = 3)
```
