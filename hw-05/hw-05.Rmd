---
title: "36-402 Homework 5"
author:
- Eu Jing Chua
- eujingc
date: "February 19, 2019"
output:
  pdf_document: default
header-includes:
    - \usepackage{enumerate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\newcommand{\est}[1]{\hat{#1}}
\newcommand{\betah}[1]{\est{\beta_{#1}}}
\newcommand{\avg}[1]{\overline{#1}}
\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var} \left[ #1 \right]}
\newcommand{\Cov}[1]{\text{Cov} \left[ #1 \right]}
\newcommand{\X}{\mathbb{X}}
\newcommand{\sumTo}[1]{\sum^{#1}_{i=1}}
\newcommand{\sumjTo}[1]{\sum^{#1}_{j=1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\txtsp}{\hspace{0.05in}}

# Question 1

**Q1 a)**

\begin{align}
\frac{1}{2h} \int_{x_0 - h}^{x_0 + h} (m + t(x - x_0) + c(x - x_0)^2) dx
    &= \frac{1}{2h} \int_{-h}^{h} (m + tu + cu^2) du, \txtsp u = x - x_0 \\
    &= \frac{1}{2h} \left[ mu + \frac{t}{2}u^2 + \frac{c}{3}u^3 \right]_{-h}^{h} \\
    &= \frac{1}{2h} (2mh + \frac{2}{3}ch^3) \\
    &= m + \frac{1}{3}ch^2 \\
    &\implies k = \frac{1}{3}
\end{align}

**Q1 b)**

\begin{align}
\E{m(X)} &= \int_{x_0 - h}^{x_0 + h} m(x) \frac{1}{2h} dx \\
    &= \frac{1}{2h} \int_{x_0 - h}^{x_0 + h} m(x) dx \\
    &\approx \frac{1}{2h} \int_{x_0 - h}^{x_0 + h} m(x_0) + m'(x_0) (x - x_0) + \frac{1}{2} m''(x_0)(x - x_0)^2 dx \\
    &= m(x_0) + \frac{m''(x_0)}{3} h^2
\end{align}

**Q1 c)**
Let $A_{x_0} = \{ i: |X_i - x_0| \le h \}$, so $\est{\mu}(x_0) = \frac{1}{|A_{x_0}|}\sum_{i \in A_{x_0}} Y_i$
\begin{align}
    \E{\est{\mu}(x_0)} &= \E{\frac{1}{|A_{x_0}|}\sum_{i \in A_{x_0}} Y_i} \\
        &= \E{\frac{1}{|A_{x_0}|}\sum_{i \in A_{x_0}} \mu(X_i) + \epsilon_i} \\
        &= \frac{1}{|A_{x_0}|}\sum_{i \in A_{x_0}} \E{\mu(X_i)} + \E{\epsilon_i} \\
        &= \E{\mu(X) \mid |X - x_0| \le h} + \E{\epsilon \mid |X - x_0| \le h} \\
        &\approx \mu(x_0) + \frac{\mu''(x_0)}{3} h^2 + 0 \\
        &= \mu(x_0) + O(h^2) \\
    \text{Bias}[\est{\mu}(x_0)] &= \E{\est{\mu}(x_0)} - \mu(x_0) \\
        &\approx O(h^2)
\end{align}

**Q1 d)**
\begin{align}
\E{m(X)} &= \int_{x_0 - h}^{x_0 + h} m(x) f(x) dx \\
    &\approx \int_{x_0 - h}^{x_0 + h} \left( m(x_0) + m'(x_0)(x - x_0) + \frac{1}{2} m''(x_0)(x - x_0)^2 \right) \left(f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2} f''(x_0)(x - x_0)^2 \right) dx \\
    &\approx \int_{x_0 - h}^{x_0 + h} \left( m(x_0) f(x_0) + \left(\frac{1}{2} m(x_0)f''(x_0) + m'(x_0)f'(x_0) + \frac{1}{2} m''(x_0) f(x_0) \right) (x - x_0)^2 \right) dx \\
    &= 2h m(x_0) f(x_0) + \frac{m(x_0)f''(x_0) + 2 m'(x_0)f'(x_0) + m''(x_0) f(x_0)}{3} h^3 \\
    &\text{Let $B$ be the event that $|X - x_0| \le h$} \\
&f(x \mid B) \approx \frac{f(x)}{2h f(x_0)} \\
&f'(x \mid B) \approx \frac{f'(x)}{2h f(x_0)} \\
&f''(x \mid B) \approx \frac{f''(x)}{2h f(x_0)} \\
\E{\est{\mu}(x_0)} &= \E{\mu(X) \mid B} \txtsp \text{as shown in c)} \\
    &= 2h \mu(x_0) f(x_0 \mid B) + \frac{\mu(x_0)f''(x_0 \mid B) + 2 \mu'(x_0)f'(x_0 \mid B) + \mu''(x_0) f(x_0 \mid B)}{3} h^3 \\
    &\approx \mu(x_0) + \frac{\mu(x_0) f''(x_0) + 2 \mu'(x_0) f'(x_0) + \mu''(x_0) f(x_0)}{6 f(x_0)} h^2 \\
    &= \mu(x_0) + O(h^2) \\
\text{Bias}[\est{\mu}(x_0)] &= \E{\est{\mu}(x_0)} - \mu(x_0) \\
        &\approx O(h^2)
\end{align}

# Question 2

**Q2 a)**

Let $u = x_1 - x_{01}$ and $v = x_2 - x_{02}$.
\begin{align}
&\frac{1}{(2h)^2} \int_{B_h} u(\vec{x}) d\vec{x} \\
&= \frac{1}{(2h)^2} \int_{x_{01} - h}^{x_{01} + h} \int_{x_{02} - h}^{x_{02} + h} u(\vec{x}) d\vec{x} \\
&= \frac{1}{(2h)^2} \int_{-h}^{h} \int_{-h}^{h} (m + t_1 u + t_2 v + c_1 u^2 + c_2 v^2 + c_3 u v) du dv \\
&= \frac{1}{(2h)^2} \int_{-h}^{h} (2h m + 2h t_2 v + \frac{2 c_1}{3} h^3 + 2h c_2 v^2) dv \\
&= \frac{1}{2h} \int_{-h}^{h} (m + t_2 v + \frac{c_1}{3} h^2 + c_2 v^2) dv \\
&= \frac{1}{2h} (2h m + \frac{2 c_1}{3} h^3 + \frac{2 c_2}{3} h^3) \\
&= m + \left( \frac{c_1}{3} + \frac{c_2}{3} \right) h^2
\end{align}

Hence $k_1 = \frac{1}{3}$ and $k_2 = \frac{1}{3}$ and $k_3 = 0$.

**Q2 b)**

\begin{align}
    m(\vec{x}) &\approx m(\vec{x_0}) + (x_1 - x_{01}) m_{x_1}(\vec{x_0}) + (x_2 - x_{02}) m_{x_2}(\vec{x_0}) + \frac{1}{2}\left( (x_1 - x_{01})^2 m_{x_1 x_1}(\vec{x_0}) + (x_2 - x_{02})^2 m_{x_2 x_2}(\vec{x_0}) \right) \\
     &+ (x_1 - x_{01})(x_2 - x_{02}) m_{x_1 x_2}(\vec{x_0}) \\
    \E{m(\vec{X})} &= \int_{B_h} m(\vec{x}) f(\vec{x}) d\vec{x} \\
        &= \frac{1}{(2h)^2} \int_{B_h} m(\vec{x}) d\vec{x} \\
        &\approx m(\vec{x_0}) + \frac{1}{3} \left( \frac{m_{x_1 x_1}(\vec{x_0})}{2} + \frac{m_{x_2 x_2}(\vec{x_0})}{2} \right) h^2 \\
        &= m(\vec{x_0}) + O(h^2)
\end{align}

**Q2 c)**\
Let $A_{\vec{x}_0} = \{ i: |X_{i1} - x_{01}| \le h \land |X_{i2} - x_{02}| \le h \}$, so $\est{\mu}(x_0) = \frac{1}{|A_{\vec{x}_0}|}\sum_{i \in A_{\vec{x}_0}} Y_i$
\begin{align}
\E{\est{\mu}(\vec{x_0})} &= \E{\frac{1}{|A_{\vec{x}_0}|}\sum_{i \in A_{\vec{x}_0}} Y_i} \\
    &= \E{\frac{1}{|A_{\vec{x}_0}|}\sum_{i \in A_{\vec{x}_0}} \mu(\vec{X}_i) + \epsilon_i} \\
    &= \frac{1}{|A_{\vec{x}_0}|}\sum_{i \in A_{\vec{x}_0}} \E{\mu(\vec{X}_i)} + \E{\epsilon_i} \\
    &= \E{\mu(\vec{X}) \mid |X_{1} - x_{01}| \le h \land |X_{2} - x_{02}| \le h} + \E{\epsilon \mid |X_{1} - x_{01}| \le h \land |X_{2} - x_{02}| \le h} \\
    &\approx \mu(\vec{x}_0) + O(h^2) \\
\text{Bias}[\est{\mu}(\vec{x}_0)] &= \E{\est{\mu}(\vec{x}_0)} - \mu(\vec{x}_0) \\
        &\approx O(h^2)
\end{align}

**Q3 a)**

\begin{align}
P(\vec{X} \in B_h) &= \int_{B_h} f(\vec{x}) d\vec{x} \\
    &\approx \int_{B_h} f(\vec{x}_0) d\vec{x} \\
    &= f(\vec{x}_0)(2h)^p
\end{align}

This approximation is only valid for small $h$, and if the pdf was smooth. Then, we can approximate the surface around $\vec{x}_0$ as "flat", having the same density as the center, which is $f(\vec{x}_0)$. The integral just becomes this average density multiplied by the $p$-dimensional volume of $B_h$.

**Q3 b)**

By linearity of expectations, if $P(\vec{x} \in B_h) \approx f(\vec{x}_0)(2h)^p$ for one point, then for n samples, the expected number of points $N$ is:

\begin{align}
N &= \sum_{i = 1}^n N_i \\
\E{N_i} &= 1 \times P(\vec{X}_i \in B_h) + 0 \times (1 - P(\vec{X}_i \in B_h)) = P(\vec{X}_i \in B_h) \\
\E{N} &= \sum_{i = 1}^n \E{N_i} \\
    &\approx n f(\vec{x}_0)(2h)^p
\end{align}

**Q3 c)**

Let $A_{\vec{x}_0} = \{ i: \vec{X}_i \in B_h \}$, where $N = |A_{\vec{x}_0}|$, so $\est{\mu}(x_0) = \frac{1}{|A_{\vec{x}_0}|}\sum_{i \in A_{\vec{x}_0}} Y_i$
\begin{align}
\Var{\est{\mu}(\vec{x}_0)} &= \frac{1}{N^2} \Var{\sum_{i \in A_{\vec{x}_0}} \mu(\vec{x}_i) + \epsilon_i} \\
    &= \frac{1}{N^2} \sum_{i \in A_{\vec{x}_0}} \Var{\epsilon_i} \\
    &= \frac{\sigma^2}{N}, \txtsp \text{assuming that for all $i$, $\Var{\epsilon_i} = \sigma^2$} \\
    &\approx \frac{\sigma^2}{n f(\vec{x}_0)(2h)^p} \\
    &= O(n^{-1}h^{-p})
\end{align}

# Question 4

**8.3 1)**

\begin{align}
f(x_1, x_2, \hdots, x_i, \hdots, x_p) - f(x_1, x_2, \hdots, x_i + c, \hdots, x_p) &= \left( a + f_i(x_i) + \sum_{j \ne i}^p f_j(x_j) \right) - \left( a + f_i(x_i + c) + \sum_{j \ne i}^p f_j(x_j) \right) \\
    &= f_i(x_i) - f_i(x_i + c)
\end{align}

**8.3 2)**

Let $g(x) = f(x, x_2)$ and $h(x) = f(x, x_2')$ for fixed $x_2$ and $x_2'$.
\begin{align}
g(x) - h(x) &= \left(a + f_1(x) + f_2(x_2) \right) - \left(a + f_1(x) + f_2(x_2') \right) \\
    &= f_2(x_2) - f_2(x_2')
\end{align}
This difference is a constant, as it does not depend on $x$.

**8.3 3)**

Let $g(x) = f(x_1, \hdots, x, \hdots, x_p)$ and $h(x) = f(x_1', \hdots, x, \hdots, x_p')$, where both $g(x), h(x)$ only vary by the $i^{th}$ coordinate, and we fix all other coordinates as $x_j$ and $x_j'$ for $g(x)$ and $h(x)$ respectively.

Then the difference along the $i^{th}$ coordinate between the two functions is
\begin{align}
g(x) - h(x) &= \left(a + f_i(x) + \sum_{j \ne i}^p f_j(x_j) \right) - \left(a + f_i(x) + \sum_{j \ne i}^p f_j(x_j') \right) \\
    &= \sum_{j \ne i}^p f_j(x_j) - f_j(x_j')
\end{align}

We know that all the curves formed by fixing one coordinate only have a difference that is independent of the coordinate being fixed (constant with respect to $x_i$), and hence are all parallel.

# Question 5

**a)**

```{r}
library(gamair)
library(np)
options(np.messages = FALSE)
data(chicago)
```

```{r cache=TRUE}
npreg.fit <- npreg(death ~ tmpd, data = chicago, bwmethod = "cv.ls")
```
```{r}
sorted.idx <- order(chicago$tmpd)
plot(death ~ tmpd, data = chicago, col = "lightgrey",
     xlab = "Temperature (F)", ylab = "Deaths",
     main = "Plot of deaths against temperature (F)")
lines(chicago$tmpd[sorted.idx], fitted(npreg.fit)[sorted.idx])
```

A kernel regression is used to fit a non-parametric regression of deaths on temperature. Least-squares cross-validation was used to determine an optimal bandwidth for kernel regession.

The plot looks relatively smooth, being quite linear with a negative relationship for most of the temperature range up till past around 70F, where the number of deaths with each increase in temperature increases.


**Q5 b)**

```{r}
library(mgcv)
am.fit <- gam(death ~ s(tmpd) + s(pm10median) + s(o3median) + s(so2median),
              data = chicago)
par(mfrow = c(2, 2))
plot(am.fit)
```
The partial response function for temperature decreases from -20F to around 70F, before it sharply increases again. The partial response functions for pm10median, o3median, and so2median all look relatively linear and smooth, with all three having positive slopes.

**Q5 c)**

The shape of the partial response function for temperature roughly matches the shape of the curve, as both curves are decreasing up till around 70F before increasing again. Both curves should have roughly the same shape, as both methods of fitting a curve non-parametrically via kernel regression and splines do not make strong assumptions about the true $\mu(x)$ but instead fit by finding a good hyperparameter via cross-validation.

**Q5 d)**

```{r}
library(knitr)
results <- c(npreg.fit$bws$fval, am.fit$gcv.ubre)
names(results) <- c("Kernel Reg.", "GAM")
kable(results, digits = 5, caption = "Cross-validated MSE")
```

Since the additive model has a lower MSE from cross-validation, we conclude that it predicts better.
